{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/24sureshkumar/TransArt-A-Multimodal-Application-for-Vernacular-Language-Translation-and-Image-Synthesis/blob/main/TRANSART_MULTIMODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwTryaq3a7Ta",
        "outputId": "200edd0b-3b44-480d-ebcb-0c5a25e871ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit pyngrok transformers diffusers accelerate sentencepiece nltk Pillow rouge_score sacrebleu --quiet\n",
        "# !pip install streamlit pyngrok transformers diffusers Pillow torch --quiet\n",
        "# !pip install bert-score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "A37aQuh5eih3",
        "outputId": "e17e61f2-4bba-4e5b-9af6-0c6b5311272e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit \\\n",
        "            transformers \\\n",
        "            torch \\\n",
        "            diffusers \\\n",
        "            Pillow \\\n",
        "            rouge_score \\\n",
        "            sentencepiece \\\n",
        "            scipy \\\n",
        "            pyngrok \\\n",
        "            accelerate \\\n",
        "            nltk \\\n",
        "            sacrebleu \\\n",
        "            git+https://github.com/openai/CLIP.git --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIJ7BhLKeBKG",
        "outputId": "d060b17d-292f-4958-acce-887800e7447b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import GPT2LMHeadModel\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from rouge_score import rouge_scorer\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "import os\n",
        "import time\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load translation model\n",
        "translator_model = MBartForConditionalGeneration.from_pretrained(\n",
        "    \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        ").to(device)\n",
        "translator_tokenizer = MBart50TokenizerFast.from_pretrained(\n",
        "    \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        ")\n",
        "translator_tokenizer.src_lang = \"ta_IN\"\n",
        "\n",
        "# Load GPT-2 for creative text\n",
        "gen_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "gen_model.eval()\n",
        "\n",
        "# Load Stable Diffusion 2.1\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-2-1\",\n",
        "    use_auth_token=os.getenv(\"HF_TOKEN\"),\n",
        "    torch_dtype=torch.float32,\n",
        ").to(device)\n",
        "pipe.safety_checker = None\n",
        "\n",
        "# Load CLIP for similarity\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# --- Functions ---\n",
        "def translate_tamil_to_english(text, reference=None):\n",
        "    start = time.time()\n",
        "    inputs = translator_tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    outputs = translator_model.generate(\n",
        "        **inputs,\n",
        "        forced_bos_token_id=translator_tokenizer.lang_code_to_id[\"en_XX\"]\n",
        "    )\n",
        "    translated = translator_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    duration = round(time.time() - start, 2)\n",
        "\n",
        "    rouge_l = None\n",
        "    if reference:\n",
        "        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "        score = scorer.score(reference.lower(), translated.lower())\n",
        "        rouge_l = round(score[\"rougeL\"].fmeasure, 4)\n",
        "\n",
        "    return translated, duration, rouge_l\n",
        "\n",
        "def generate_creative_text(prompt, max_length=100):\n",
        "    start = time.time()\n",
        "    input_ids = gen_tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    output = gen_model.generate(input_ids, max_length=max_length, do_sample=True, top_k=50, temperature=0.9)\n",
        "    text = gen_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    duration = round(time.time() - start, 2)\n",
        "\n",
        "    tokens = text.split()\n",
        "    repetition_rate = sum(t1 == t2 for t1, t2 in zip(tokens, tokens[1:])) / len(tokens)\n",
        "\n",
        "    # Perplexity\n",
        "    with torch.no_grad():\n",
        "        input_ids = gen_tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
        "        outputs = gen_model(input_ids, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "        perplexity = torch.exp(loss).item()\n",
        "\n",
        "    return text, duration, len(tokens), round(repetition_rate, 4), round(perplexity, 4)\n",
        "\n",
        "def generate_image(prompt):\n",
        "    try:\n",
        "        start = time.time()\n",
        "        result = pipe(prompt)\n",
        "        image = result.images[0].resize((256, 256))\n",
        "        tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\")\n",
        "        image.save(tmp_file.name)\n",
        "        duration = round(time.time() - start, 2)\n",
        "        return tmp_file.name, duration, image\n",
        "    except Exception as e:\n",
        "        return None, 0, f\"Image generation failed: {str(e)}\"\n",
        "\n",
        "def evaluate_clip_similarity(text, image):\n",
        "    inputs = clip_processor(text=[text], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "        logits_per_image = outputs.logits_per_image\n",
        "        probs = F.softmax(logits_per_image, dim=1)\n",
        "        similarity_score = logits_per_image[0][0].item()\n",
        "    return round(similarity_score, 4)\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.set_page_config(page_title=\"Tamil → English + AI Art\", layout=\"centered\")\n",
        "st.title(\"🧠 Tamil → English + 🎨 Creative Text + AI Image\")\n",
        "\n",
        "tamil_input = st.text_area(\"✍️ Enter Tamil text here\", height=150)\n",
        "reference_input = st.text_input(\"📘 Optional: Reference English translation for ROUGE\")\n",
        "\n",
        "if st.button(\"🚀 Generate Output\"):\n",
        "    if not tamil_input.strip():\n",
        "        st.warning(\"Please enter Tamil text.\")\n",
        "    else:\n",
        "        with st.spinner(\"🔄 Translating Tamil to English...\"):\n",
        "            english_text, t_time, rouge_l = translate_tamil_to_english(tamil_input, reference_input)\n",
        "\n",
        "        st.success(f\"✅ Translated in {t_time} seconds\")\n",
        "        st.markdown(f\"**📝 English Translation:** `{english_text}`\")\n",
        "        if rouge_l is not None:\n",
        "            st.markdown(f\"📊 **ROUGE-L Score:** `{rouge_l}`\")\n",
        "        else:\n",
        "            st.info(\"ℹ️ ROUGE-L not calculated. Reference not provided.\")\n",
        "\n",
        "        with st.spinner(\"🎨 Generating image...\"):\n",
        "            image_path, img_time, image_obj = generate_image(english_text)\n",
        "\n",
        "        if isinstance(image_obj, Image.Image):\n",
        "            st.success(f\"🖼️ Image generated in {img_time} seconds\")\n",
        "            st.image(Image.open(image_path), caption=\"AI-Generated Image\", use_column_width=True)\n",
        "\n",
        "            with st.spinner(\"🔎 Evaluating CLIP similarity...\"):\n",
        "                clip_score = evaluate_clip_similarity(english_text, image_obj)\n",
        "                st.markdown(f\"🔍 **CLIP Text-Image Similarity:** `{clip_score}`\")\n",
        "        else:\n",
        "            st.error(image_obj)\n",
        "\n",
        "        with st.spinner(\"💡 Generating creative text...\"):\n",
        "            creative, c_time, tokens, rep_rate, ppl = generate_creative_text(english_text)\n",
        "\n",
        "        st.success(f\"✨ Creative text generated in {c_time} seconds\")\n",
        "        st.markdown(f\"**🧠 Creative Output:** `{creative}`\")\n",
        "        st.markdown(f\"📌 Tokens: `{tokens}`, 🔁 Repetition Rate: `{rep_rate}`, 📉 Perplexity: `{ppl}`\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.caption(\"Built by Sureshkumar R using MBart, GPT-2, Stable Diffusion 2.1, and CLIP on Hugging Face 🤗\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9bSrUI4eA_R",
        "outputId": "17a6914a-b2f4-4b47-8f08-4e8b417deac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Streamlit App URL: NgrokTunnel: \"https://8d68-34-16-174-195.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "2025-07-04 12:49:42.834 \n",
            "Warning: the config option 'server.enableCORS=false' is not compatible with\n",
            "'server.enableXsrfProtection=true'.\n",
            "As a result, 'server.enableCORS' is being overridden to 'true'.\n",
            "\n",
            "More information:\n",
            "In order to protect against CSRF attacks, we send a cookie with each request.\n",
            "To do so, we must specify allowable origins, which places a restriction on\n",
            "cross-origin resource sharing.\n",
            "\n",
            "If cross origin resource sharing is required, please disable server.enableXsrfProtection.\n",
            "            \n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.16.174.195:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-07-04 12:50:20.750982: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751633420.942672   32862 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751633421.002257   32862 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-04 12:50:21.239302: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model_index.json: 100% 537/537 [00:00<00:00, 3.06MB/s]\n",
            "Fetching 13 files:   0% 0/13 [00:00<?, ?it/s]\n",
            "scheduler_config.json: 100% 345/345 [00:00<00:00, 1.48MB/s]\n",
            "\n",
            "preprocessor_config.json: 100% 342/342 [00:00<00:00, 263kB/s]\n",
            "\n",
            "Fetching 13 files:   8% 1/13 [00:00<00:02,  5.27it/s]\n",
            "\n",
            "special_tokens_map.json: 100% 460/460 [00:00<00:00, 2.34MB/s]\n",
            "\n",
            "\n",
            "config.json: 100% 633/633 [00:00<00:00, 362kB/s]\n",
            "\n",
            "\n",
            "vocab.json: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer_config.json: 100% 824/824 [00:00<00:00, 3.39MB/s]\n",
            "merges.txt: 525kB [00:00, 13.4MB/s]\n",
            "vocab.json: 1.06MB [00:00, 22.9MB/s]\n",
            "\n",
            "text_encoder/model.safetensors:   0% 0.00/1.36G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "config.json: 100% 939/939 [00:00<00:00, 5.77MB/s]\n",
            "\n",
            "\n",
            "config.json: 100% 611/611 [00:00<00:00, 2.69MB/s]\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   0% 0.00/3.46G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:   0% 0.00/335M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   0% 1.24M/3.46G [00:02<1:41:54, 566kB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:   0% 1.46M/1.36G [00:02<38:05, 595kB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   0% 5.32M/3.46G [00:03<33:54, 1.70MB/s] \u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   0% 8.04M/3.46G [00:04<30:54, 1.86MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:   2% 22.7M/1.36G [00:17<16:20, 1.37MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   0% 8.04M/3.46G [00:19<30:54, 1.86MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  20% 67.0M/335M [00:25<01:39, 2.68MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  40% 134M/335M [00:27<00:35, 5.72MB/s] \u001b[A\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:   2% 22.7M/1.36G [00:29<16:20, 1.37MB/s]\u001b[A\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  40% 134M/335M [00:39<00:35, 5.72MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   2% 75.1M/3.46G [01:19<1:00:37, 932kB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   2% 75.1M/3.46G [01:29<1:00:37, 932kB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   4% 142M/3.46G [01:30<30:37, 1.81MB/s]  \u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   6% 209M/3.46G [01:41<20:41, 2.62MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   8% 276M/3.46G [01:53<15:53, 3.34MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  10% 343M/3.46G [02:05<13:17, 3.91MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  60% 201M/335M [02:17<01:52, 1.18MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  10% 343M/3.46G [02:19<13:17, 3.91MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  80% 268M/335M [02:27<00:38, 1.75MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  11% 389M/3.46G [02:33<18:01, 2.84MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  80% 268M/335M [02:39<00:38, 1.75MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  11% 389M/3.46G [02:49<18:01, 2.84MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors: 100% 335M/335M [02:51<00:00, 1.95MB/s]\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  13% 453M/3.46G [02:52<16:32, 3.03MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  13% 453M/3.46G [03:09<16:32, 3.03MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:   7% 89.7M/1.36G [03:10<47:31, 446kB/s] \u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  15% 520M/3.46G [03:11<15:23, 3.19MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  17% 587M/3.46G [03:28<14:19, 3.35MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:   7% 89.7M/1.36G [03:29<47:31, 446kB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  19% 654M/3.46G [03:35<10:54, 4.29MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  21% 721M/3.46G [03:41<08:43, 5.24MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  21% 721M/3.46G [03:59<08:43, 5.24MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  23% 784M/3.46G [04:03<10:38, 4.20MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  25% 851M/3.46G [04:09<08:23, 5.19MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  12% 157M/1.36G [04:16<31:22, 640kB/s] \u001b[A\n",
            "text_encoder/model.safetensors:  16% 224M/1.36G [04:28<18:17, 1.04MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  27% 918M/3.46G [04:28<09:18, 4.56MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  27% 918M/3.46G [04:39<09:18, 4.56MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  16% 224M/1.36G [04:39<18:17, 1.04MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  28% 985M/3.46G [04:46<09:42, 4.26MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  30% 1.05G/3.46G [04:52<07:41, 5.22MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  21% 291M/1.36G [05:05<14:22, 1.24MB/s]\u001b[A\n",
            "text_encoder/model.safetensors:  26% 358M/1.36G [05:05<08:44, 1.92MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  30% 1.05G/3.46G [05:09<07:41, 5.22MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  26% 358M/1.36G [05:19<08:44, 1.92MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  32% 1.12G/3.46G [05:23<10:38, 3.67MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  31% 425M/1.36G [05:23<06:51, 2.28MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  34% 1.19G/3.46G [05:34<09:04, 4.18MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  31% 425M/1.36G [05:39<06:51, 2.28MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  36% 1.25G/3.46G [05:40<07:04, 5.21MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  38% 1.32G/3.46G [05:52<06:45, 5.29MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  36% 492M/1.36G [05:52<06:18, 2.30MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  38% 1.32G/3.46G [06:09<06:45, 5.29MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  36% 492M/1.36G [06:09<06:18, 2.30MB/s]\u001b[A\n",
            "text_encoder/model.safetensors:  41% 559M/1.36G [06:11<05:08, 2.60MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  40% 1.39G/3.46G [06:10<07:27, 4.64MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  42% 1.45G/3.46G [06:27<07:30, 4.46MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  44% 1.52G/3.46G [06:27<05:06, 6.34MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  41% 559M/1.36G [06:29<05:08, 2.60MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  46% 1.59G/3.46G [06:35<04:33, 6.86MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  48% 1.66G/3.46G [06:35<03:06, 9.67MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  46% 626M/1.36G [06:48<05:24, 2.27MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  48% 1.66G/3.46G [06:49<03:06, 9.67MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  50% 1.72G/3.46G [06:53<04:27, 6.52MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  46% 626M/1.36G [06:59<05:24, 2.27MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  52% 1.79G/3.46G [06:59<03:45, 7.41MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  54% 1.86G/3.46G [07:06<03:16, 8.18MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  51% 693M/1.36G [07:10<04:29, 2.48MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  54% 1.86G/3.46G [07:19<03:16, 8.18MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  56% 1.92G/3.46G [07:22<04:04, 6.31MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  56% 760M/1.36G [07:28<03:38, 2.76MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  57% 1.99G/3.46G [07:34<04:04, 6.02MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  61% 827M/1.36G [07:35<02:31, 3.53MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  59% 2.06G/3.46G [07:47<04:00, 5.84MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  61% 827M/1.36G [07:49<02:31, 3.53MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  61% 2.12G/3.46G [07:53<03:17, 6.78MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  66% 894M/1.36G [07:59<02:23, 3.25MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  63% 2.19G/3.46G [08:05<03:22, 6.28MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  66% 894M/1.36G [08:09<02:23, 3.25MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  65% 2.26G/3.46G [08:11<02:46, 7.22MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  67% 2.33G/3.46G [08:19<02:27, 7.70MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  67% 2.33G/3.46G [08:29<02:27, 7.70MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  71% 961M/1.36G [08:30<02:21, 2.83MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  69% 2.39G/3.46G [08:30<02:31, 7.09MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  71% 2.46G/3.46G [08:47<02:57, 5.65MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  75% 1.03G/1.36G [08:47<01:48, 3.07MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  73% 2.53G/3.46G [08:53<02:19, 6.71MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  75% 2.59G/3.46G [08:53<01:31, 9.46MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  77% 2.66G/3.46G [08:54<01:02, 12.8MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  80% 1.09G/1.36G [08:54<01:09, 3.85MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  79% 2.73G/3.46G [09:02<01:06, 11.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  81% 2.79G/3.46G [09:04<00:47, 14.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  83% 2.86G/3.46G [09:05<00:32, 18.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  85% 2.93G/3.46G [09:05<00:21, 24.6MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  80% 1.09G/1.36G [09:09<01:09, 3.85MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  86% 2.99G/3.46G [09:11<00:24, 19.4MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  85% 1.16G/1.36G [09:11<00:51, 3.88MB/s]\u001b[A\n",
            "text_encoder/model.safetensors:  90% 1.23G/1.36G [09:17<00:27, 4.84MB/s]\u001b[A\n",
            "text_encoder/model.safetensors:  95% 1.29G/1.36G [09:17<00:09, 6.85MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  88% 3.06G/3.46G [09:29<00:47, 8.42MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  95% 1.29G/1.36G [09:29<00:09, 6.85MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  90% 3.13G/3.46G [09:29<00:28, 11.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  92% 3.20G/3.46G [09:36<00:23, 11.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  92% 3.20G/3.46G [09:49<00:23, 11.3MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors: 100% 1.36G/1.36G [10:02<00:00, 2.26MB/s]\n",
            "Fetching 13 files:  38% 5/13 [10:02<17:05, 128.23s/it]\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  94% 3.26G/3.46G [10:08<00:41, 4.89MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  96% 3.33G/3.46G [10:14<00:23, 5.82MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  98% 3.40G/3.46G [10:14<00:08, 8.23MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors: 100% 3.46G/3.46G [10:20<00:00, 5.58MB/s]\n",
            "Fetching 13 files: 100% 13/13 [10:21<00:00, 47.79s/it]\n",
            "Keyword arguments {'use_auth_token': None} are not expected by StableDiffusionPipeline and will be ignored.\n",
            "Loading pipeline components...: 100% 6/6 [00:02<00:00,  2.35it/s]\n",
            "config.json: 4.19kB [00:00, 1.53MB/s]\n",
            "pytorch_model.bin: 100% 605M/605M [00:35<00:00, 17.1MB/s]\n",
            "model.safetensors:   0% 0.00/605M [00:00<?, ?B/s]Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "\n",
            "preprocessor_config.json: 100% 316/316 [00:00<00:00, 1.71MB/s]\n",
            "\n",
            "tokenizer_config.json: 100% 592/592 [00:00<00:00, 3.36MB/s]\n",
            "\n",
            "vocab.json: 862kB [00:00, 9.88MB/s]\n",
            "model.safetensors:   0% 1.81M/605M [00:01<06:43, 1.49MB/s]\n",
            "merges.txt: 525kB [00:00, 39.9MB/s]\n",
            "model.safetensors:  17% 105M/605M [00:01<00:04, 118MB/s]  \n",
            "tokenizer.json: 2.22MB [00:00, 37.7MB/s]\n",
            "\n",
            "special_tokens_map.json: 100% 389/389 [00:00<00:00, 1.50MB/s]\n",
            "model.safetensors: 100% 605M/605M [01:04<00:00, 9.33MB/s]\n",
            "Keyword arguments {'use_auth_token': None} are not expected by StableDiffusionPipeline and will be ignored.\n",
            "Loading pipeline components...: 100% 6/6 [00:00<00:00,  8.41it/s]\n",
            "Keyword arguments {'use_auth_token': None} are not expected by StableDiffusionPipeline and will be ignored.\n",
            "Loading pipeline components...:   0% 0/6 [00:00<?, ?it/s]\u001b[31m──\u001b[0m\u001b[31m────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────────\u001b[0m\u001b[31m──\u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1;33mexec_code.py\u001b[0m: \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[94m128\u001b[0m in \u001b[92mexec_func_with_error_handling\u001b[0m                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1;33mscript_runner\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m669\u001b[0m in \u001b[92mcode_to_exec\u001b[0m                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/content/\u001b[0m\u001b[1;33mapp.py\u001b[0m:\u001b[94m30\u001b[0m in \u001b[92m<module>\u001b[0m                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 27 \u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 28 \u001b[0m\u001b[2m# Load GPT-2 for creative text\u001b[0m                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 29 \u001b[0mgen_tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[0m\u001b[33mgpt2\u001b[0m\u001b[33m\"\u001b[0m)                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 30 gen_model = \u001b[1;4mAutoModelForCausalLM.from_pretrained(\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mgpt2\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m).to(device)\u001b[0m            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 31 \u001b[0mgen_model.eval()                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 32 \u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 33 \u001b[0m\u001b[2m# Load Stable Diffusion 2.1\u001b[0m                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m4106\u001b[0m in \u001b[92mto\u001b[0m    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4103 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4104 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[0m\u001b[33m\"\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4105 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m4106 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4;96msuper\u001b[0m\u001b[1;4m().to(*args, **kwargs)\u001b[0m                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4107 \u001b[0m\u001b[2m│   \u001b[0m                                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4108 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mhalf\u001b[0m(\u001b[96mself\u001b[0m, *args):                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4109 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Checks if the model is quantized\u001b[0m                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1343\u001b[0m in \u001b[92mto\u001b[0m        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1340 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1341 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m                                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1342 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1343 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4;96mself\u001b[0m\u001b[1;4m._apply(convert)\u001b[0m                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1344 \u001b[0m\u001b[2m│   \u001b[0m                                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1345 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mregister_full_backward_pre_hook\u001b[0m(                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1346 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m903\u001b[0m in \u001b[92m_apply\u001b[0m     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 900 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92m_apply\u001b[0m(\u001b[96mself\u001b[0m, fn, recurse=\u001b[94mTrue\u001b[0m):                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 901 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m recurse:                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 902 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.children():                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 903 \u001b[2m│   │   │   │   \u001b[0m\u001b[1;4mmodule._apply(fn)\u001b[0m                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 904 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 905 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mcompute_should_use_set_data\u001b[0m(tensor, tensor_applied):              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 906 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m torch._has_compatible_shallow_copy_type(tensor, tensor_applied \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m903\u001b[0m in \u001b[92m_apply\u001b[0m     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 900 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92m_apply\u001b[0m(\u001b[96mself\u001b[0m, fn, recurse=\u001b[94mTrue\u001b[0m):                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 901 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m recurse:                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 902 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.children():                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 903 \u001b[2m│   │   │   │   \u001b[0m\u001b[1;4mmodule._apply(fn)\u001b[0m                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 904 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 905 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mcompute_should_use_set_data\u001b[0m(tensor, tensor_applied):              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 906 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m torch._has_compatible_shallow_copy_type(tensor, tensor_applied \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m930\u001b[0m in \u001b[92m_apply\u001b[0m     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 927 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# track autograd history of `param_applied`, so we have to use\u001b[0m    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 928 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# `with torch.no_grad():`\u001b[0m                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 929 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 930 \u001b[2m│   │   │   │   \u001b[0mparam_applied = \u001b[1;4mfn(param)\u001b[0m                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 931 \u001b[0m\u001b[2m│   │   │   \u001b[0mp_should_use_set_data = compute_should_use_set_data(param, param_ \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 932 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 933 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# subclasses may have multiple child tensors so we need to use sw\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1336\u001b[0m in \u001b[92mconvert\u001b[0m   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1333 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1334 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mNotImplementedError\u001b[0m \u001b[94mas\u001b[0m e:                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1335 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mstr\u001b[0m(e) == \u001b[33m\"\u001b[0m\u001b[33mCannot copy out of meta tensor; no data!\u001b[0m\u001b[33m\"\u001b[0m:      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1336 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mNotImplementedError\u001b[0m\u001b[1;4m(\u001b[0m                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1337 \u001b[0m\u001b[1;2;4m│   │   │   │   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4me\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m Please use torch.nn.Module.to_empty() instead o\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1338 \u001b[0m\u001b[1;2;4m│   │   │   │   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mwhen moving module from meta to a different device.\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1339 \u001b[0m\u001b[1;2;4m│   │   │   │   │   \u001b[0m\u001b[1;4m) \u001b[0m\u001b[1;4;94mfrom\u001b[0m\u001b[1;4;90m \u001b[0m\u001b[1;4;94mNone\u001b[0m                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[1;91mNotImplementedError: \u001b[0mCannot copy out of meta tensor; no data! Please use \n",
            "\u001b[1;35mtorch.nn.Module.to_empty\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m instead of \u001b[1;35mtorch.nn.Module.to\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m when moving module from meta \n",
            "to a different device.\n",
            "Loading pipeline components...: 100% 6/6 [00:02<00:00,  2.77it/s]\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "Exception ignored in: <module 'threading' from '/usr/lib/python3.11/threading.py'>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1541, in _shutdown\n",
            "    def _shutdown():\n",
            "    \n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 43, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/server/server.py\", line 458, in stop\n",
            "    self._runtime.stop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/runtime.py\", line 324, in stop\n",
            "    async_objs.eventloop.call_soon_threadsafe(stop_on_eventloop)\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 807, in call_soon_threadsafe\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 520, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "Exception ignored in atexit callback: <function dump_compile_times at 0x7ba0fa16d940>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
            "    log.info(compile_times(repr=\"str\", aggregate=True))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
            "    out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\", line 207, in tabulate\n",
            "    import tabulate\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1138, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 1070, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap>\", line 1030, in __exit__\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 43, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/server/server.py\", line 458, in stop\n",
            "    self._runtime.stop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/runtime.py\", line 324, in stop\n",
            "    async_objs.eventloop.call_soon_threadsafe(stop_on_eventloop)\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 807, in call_soon_threadsafe\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 520, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers diffusers accelerate streamlit pyngrok --quiet\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "import os\n",
        "os.environ[\"NGROK_AUTHTOKEN\"] = \"your ngrok authtoken\"\n",
        "!ngrok config add-authtoken $NGROK_AUTHTOKEN\n",
        "\n",
        "# Run Streamlit app\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Streamlit App URL:\", public_url)\n",
        "!streamlit run app.py --server.port 8501 --server.enableCORS false"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOV9PfanexNZLjHTSVXOdiq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}